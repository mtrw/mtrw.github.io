---
title: "The glorious R::data.table"
output:
    html_document:
      toc_depth: 3
editor_options: 
  chunk_output_type: console
---

```{r,label=setup,include=F}
require(data.table)
options(datatable.print.nrows = 10,datatable.print.topn = 3)
```

---

# What is `data.table`?

`data.table` is an R package designed to make working with data as easy and efficient as it can possibly be. At a first glance it appears to update the standard R object `data.frame`. In case you haven't used them, you can still follow this tutorial. Basically `data.frame`s are R's most flexible and broadly used way to store data with a 'spreadsheet' structure, i.e., rows and columns.

But `data.table` adds a lot of functionality besides just making data handling infinitely better. And it takes (and perfects) ideas from all the best data handling tools:

- You can print tables on screen elegantly better than `tibble`.
- You can manipulate rows and columns and apply functions over them (grouping by factors etc.) better than `dplyr`.
- You can effectively run loops better than the various `*ply` functions.
- You can chain commands like `magrittr` (though ... `magrittr` and [native pipes](https://ivelasq.rbind.io/blog/understanding-the-r-pipe/) are still great and work excellently in tandem with `data.table`)
- You can query tables with all the power of `SQL`.
- You can merge and join tables with the best of them, including very useful features like [rolling joins](https://www.r-bloggers.com/2016/06/understanding-data-table-rolling-joins/).
- You can cast and melt tables better than `reshape2`.
- You can find "overlaps" between regions e.g. of genomic intervals, like with `GRanges` ...

And because it does all this so elegantly, it is **super easy to learn**. To learn the most important stuff, let's run an analysis on some Formula 1 racing data, because, of course.

Follow along in an [Rstudio session](./tutorial_Rstart.Rmd). Make sure you run the commands, and most importantly, *experiment with them*.

<figure>
  <img src="images/f1.jpg" title="Ferrari make famously bad strategic decisions. In contrast, learning `data.table` is a flawless race strategy for life." style="width:60%">
  <figcaption>By the time you've finished this tutorial you will be wrangling data faster than Charles Leclerc can drive this Ferrari.</figcaption>
</figure>

---

# Installation / loading

```{r,eval=F}
#If not already installed: install.packages("data.table")
require(data.table)
```

---

# Reading and writing data

The data we'll work with are from [here](https://www.kaggle.com/datasets/rohanrao/formula-1-world-championship-1950-2020). You can also find it on the Spartan storage server at /data/gpfs/projects/punim1869/shared_data/bioinfo_tutorials/f1.

## Text-based table formats

Most text-based ways of storing data consist of lines of text (one per row), and a separation character delimiting the column entries of each row. For example, in the **csv** ([c]omma-[s]eparated [v]alue) format, `"a","row","with",7,"columns","would","look","like","this"`. Note that in X-separated formats, quote marks are often used to indicate when a column contains text (c.f., say, numbers) because, to a computer, numbers and the strings of text characters that represent them are very different things.

The alternative to text-based data storage is binary files, which is a topic for another day.

## fread()

`data.table` has its own table parsing function, `fread()` ([f]ast [read]). It assumes you are giving it some X-separated table and investigates the top of a the file (10,000 lines by default) and tries to work out what the separator is and what the kind of data in each column is, then reads it. Use it to read in the data about lap times, drivers, and races---something like the following:

```{r}
lt <- fread("data/lapTimes.csv")
dr <- fread("data/drivers.csv")
ra <- fread("data/races.csv")
```

The variables `dr` and `lt` now contain `data.table` objects. Since these are enhanced `data.frame`s, we can do data.frame type things with them, such as printing the contents by running the variable name alone:

```{r}
lt
```

Notice it prints in a useful format showing the top and bottom few rows (default is five but you can [play with it](https://jangorecki.gitlab.io/data.cube/library/data.table/html/print.data.table.html)).

There are some useful arguments you can give to `fread()` as necessary:

- `fread(header=F)`
  - Often your tables won't have a top row containing names. In this case, tell `fread()` not to assume the top row is column labels. It will call the columns *"V1"*, *"V2"*, ... *"Vn"* by default, unless ...
- `fread(col.names=c("names","for","columns","go","in","this","vector"))`
  - Obviously, for this to work the vector of column names must equal the number of columns.
- `fread(select=1:3)`
  - This would read only the first three columns. With `select=c(6,3,5)` we would get those columns in that order.

## Writing data

Is done with the normal R functions like `write.csv` and `saveRDS`.

# The central concept: Three indexing fields

A `data.table` is primarily manipulated using a square-bracket syntax similar to `data.frame`s. So you can, for example, provide indices for rows and columns in the first two arguments to select them, just like regular `data.frame`s:

```{r}
lt[5,]
lt[1:5]
lt[,1]
lt[,"lap"]
lt[,c("lap","time")]
```

And you can always use the dollar sign operator `$` to extract a column as a vector ...

```{r}
dr$forename[1:10]
# or perhaps you prefer ...
dr[1:10]$forename
```

If you want to select specific rows based on some property, you *could* use logical vectors (vectors of TRUEs and FALSEs that indicate whether a row should be selected):

```{r}
selectMe <- lt$lap < 3
selectMe[1:10]
lt[selectMe,] # Note now only data for laps 1 and 2 are there
rm(selectMe)  # It's good to clean up variables you don't need later to save memory
# Four steps ... really?
```

... In reality, these operations are rarely used. `data.table`'s natural method of doing things is almost always better. Instead, we think of the indexing operator as having three fields, that we use for different purposes. We will learn them in order, but also pick up some other tricks on the way. To begin with, memorise this:

> dataTable[ \<SELECT\> , \<DO_SOMETHING\> , \<GROUP_BY\> ]

## The [ \<SELECT\> ,,] field

While you're working in the indexing fields, you can simply refer to column names of the table directly, and they will be treated as vectors. Which, as far as the computer's understanding of a table goes, is exactly what they are.

The output of whatever you write in the SELECT field is used to (surprise!) select rows. Here are some typical examples:

```{r}
lt[lap<3,]
lt[lap<3 & driverId==20,]
```

This works because (say) `lap<3` outputs a logical vector that is TRUE when lap contains 1 or 2, and FALSE otherwise. But what you output can also be numbers referring to rows. A classic example is this trick used to sort the rows by some column ...

```{r}
lt[order(milliseconds),]
```

Note the comma is optional but you should write it anyway for good reasons that you can ask me about.

Also note in Rstudio you can use `<tab>` to autocomplete column names while you work inside the indexing fields.

### Special variable: `.N`

Inside the indexing fields, `.N` always contains the number of rows of the table (or the group; see later). Hence, to reverse the table:

```{r}
lt[.N:1,]
```

### %in% and %between%

These are exceptionally useful for selection:

```{r}
lt[lap %between% c(4,7),] # Select laps between (and including) 4 and 7
lt[lap %in% c(4,6,7),] # Select laps 4, 6 and 7
lt[!lap %in% c(4,6,7),] # Select all laps except 4, 6 and 7
```

As you can imagine, this is something you might use to select, say, genes or SNPs falling within a particular range.

## Merging

The SELECT field can also be used for one of`data.table`'s most powerful functions, merging tables together. Notice in `lt` that the drivers and races are given numbers, not names.

If you look inside `ra` and `dr`, you will notice these tables allow us to see what number corresponds to what driver/race.

Merging allows us to use one `data.table` to look up what rows of a second `data.table` it matches according to the values in some column that occurs in both tables (say, driverId). When you think about it, it is almost like the rows of one table are *selecting* which rows of a second data table to join with. This is why it makes sense to use the SELECT indexing field for merging.

To do it, we just put the selecting `data.table` in the SELECT field of the other `data.table`, and then add an argument listing which column(s) to look for matches on.

```{r}
dr[lt,on=list(driverId)]
```

Note `data.table` uses lists often so the dot character `.` can be used as a shortcut for the `list` function.

Also note you can join on more than one column, just add all the names to the `on=` list.

For more flexibility, check the documentation with `?merge.data.table`. You'll find you can do cool things like return rows that do NOT match.
Let's merge all our driver and race data into the lap times table and move on.

```{r}
lt <- dr[lt,on=.(driverId)]
lt <- ra[lt,on=.(raceId)]
```

If you look at `lt` now, you'll see what happens when a column name overlaps between the tables, (besides the one being merged on; in this case, "url"). `data.table` automatically renames one of them (e.g.) "i.url".

## The [, \<DO_SOMETHING\> ,] field

The DO_SOMETHING field is where the work gets done. Whether you are manipulating or adding or deleting columns, making calculations and returning the results, or even plotting, you do it all here. For example, you could do some calculation on the columns. Treat columns just like regular vectors. That's how the computer thinks of them, so why shouldn't you too?

```{r}
#What was the fastest lap?
lt[, min(milliseconds) ]
# ... in minutes
lt[, min(milliseconds)/1000/60 ]
# ... at an Australian Grand Prix (notice we are now also using the SELECT field)
lt[ name=="Australian Grand Prix" , min(milliseconds)/1000/60 ]
# ... and what years did Sebastian Vettel race in Australia?
lt[ name=="Australian Grand Prix" , unique(year) ]
# ... so what lap was Sebastian Vettel's fastest lap at the 2011 Australian Grand Prix?
lt[ name=="Australian Grand Prix" & year==2011 & surname=="Vettel" , lap[which(milliseconds==min(milliseconds))] ]
```

### Ceci n'est pas une pipe

This last line could be cleaner. If we need to filter multiple times, we can just chain indexing operations together. This can simplify the above command as follows:

```{r}
lt[ name=="Australian Grand Prix" & year==2011 & surname=="Vettel" , ][ milliseconds==min(milliseconds), lap ]
```

Command chaining like this makes `data.table` function as a **pipe**! Just like `magrittr`! Brilliant!

I wonder how this lap compares to his other laps that day? Well, we could make a quick plot ...

```{r}
lt[ name=="Australian Grand Prix" & year==2011 & surname=="Vettel" , plot(x=lap,y=milliseconds/1000/60) ]
```

... looks like he had two pit stops and got faster and faster as the race went on. Which is good because .. he won! See?

```{r}
lt[ name=="Australian Grand Prix" & year==2011 & surname=="Vettel", ][ lap==max(lap), paste("At the end of the final lap, Vettel was in position",position,"!") ]
```

### Returning a new `data.table`

Very often, the result you want is another`data.table`. To do this, we use the <DO_SOMETHING> should return a list, in a format like `list( item1_name=<vector> , item2_name=<another vector> )` Each item in the list will become a column. Let's use it to generate some summary statistics for all the races together:

```{r}
#Remember, `.()` is a shortcut for `list()`
lt[ , .( fastest_race_lap = min(milliseconds) , number_of_drivers = length(unique(driverId)) , average_driver_hours_per_race=sum(milliseconds)/3600000/length(unique(driverId)) ) ] 
```

Note, you can clean this code using multiple lines:

```{r,eval=F}
lt[ , .(
  fastest_race_lap = min(milliseconds),
  number_of_drivers = length(unique(driverId)),
  average_driver_race_hours=sum(milliseconds)/3600000/length(unique(driverId))
)]
```

A common use is to trim your `data.table` down to just important columns. Let's use it now to make a smaller table to look at in future, one that only includes the Melbourne Grand Prix stats:

```{r}
#(l)ap (t)imes, (A)ustralian GP
lt[,unique(name)] # Just to have a look at the Grand Prix names
ltA <- lt[ name=="Australian Grand Prix", .(
  forename,
  surname,
  lap,
  milliseconds,
  position,
  name,
  circuitId,
  year
)]
```

### Adding or modifying columns

The beautifully-named *walrus operator* `:=` is used to add, modify, or delete columns.

To add:

```{r}
#add two columns using chaining
ltA[,seconds:=milliseconds/1000][,fullname:=paste(forename,surname)]
ltA
```

To modify:

```{r}
#replace spaces with underscores in $name
ltA[,name:=gsub(" ","_",name)]
ltA
```

To destroy:

```{r}
ltA[,milliseconds:=NULL]
ltA
```

## The [,, \<GROUP_BY\> ] field

We can work out the average lap time over all these races (try this yourself), but what about getting the average lap time for *each* race? We need to perform something like `mean(seconds)`, but do it separately for each unique entry in the `$year` column.

This is known as *grouping*, in this case, grouping by `$year`. And we use the `by=` argument in the <GROUP_BY> field to provide a list of columns we wish to group by.

```{r}
ltA[ , mean(seconds) , by=.(year) ]
```

Whoops! We never provided a column name so `data.table` named it "V1". Let's fix that.

```{r}
ltA[ , .( mean_laptime = mean(seconds) ) , by=.(year) ]
```

And if we want to know the lap time per year per driver, we can group on more variables ...

```{r}
ltA[ , .( mean_laptime = mean(seconds) ) , by=.(year,fullname) ]
```

... and perform multiple calculations ...

```{r}
ltA[ , .( mean_laptime = mean(seconds) ,  fastest_lap = min(seconds) ) , by=.(year,fullname) ]
```

... and of course filter rows. Congratulations, we are now using all three indexing fields together!

```{r}
ltA[ year > 2010 , .( mean_laptime = mean(seconds) ,  fastest_lap = min(seconds) ) , by=.(year,fullname) ]
```

This is a great example of how quickly `data.table` aggregates good stats for plotting. We can very quickly use the above data to look at how average lap times evolve over seasons, for example ...

```{r}
require(ggplot2)
ggplot(
  data=ltA[ , .( mean_laptime = mean(seconds) ,  fastest_lap = min(seconds) ) , by=.(year,fullname) ],
  mapping=aes(x=year,y=fastest_lap,colour=fullname)
) + geom_line() + theme(legend.position = "none")
```

## Getting slightly tricky

### Initialise a new data.table from scratch

To make a `data.table` directly in R:

```{r}
newDt <- data.table(
  colCharacter = c("h","e","l","l","o","!"),
  colInteger   = 1L:2L,
  colLogical   = c(F,F,T)
)
newDt
```

Notice the vectors you provide to fill the columns "recycle", i.e., they repeat again and again until they match the length of the longest vector. If they are not divisible evenly into that length `data.table` will ... actually that's a good exercise, try it and find out! 

### Group by with .N

Quite often you need to just count the members of each category, say, the number of laps each driver has driven. You could write ...

```{r}
ltA[,.(N=.N),by=.(surname)]
```

`data.table` offers this shortcut ...

```{r,eval=F}
ltA[,.N,by=.(surname)]
```

... which will automatically name the count column "N"

### Special variable: `.SD`

Say you want to filter your `data.table` to select the driver with the fastest lap in each race (or equivalently, filter BLAST results, taking the highest-scoring alignment for each query ... but this is a strictly F1-based tutorial).

Naturally you will think about grouping by race. And about SELECTing a row based on lap time. But the SELECT field acts on the *whole table*, and you want to select from *each group*. What to do?

When grouping, the special variable `.SD` gives you access to a mini data.table containing just the group it is working on.

```{r}
ltA[,.SD[seconds==min(seconds)],by=.(year)]
```

### `setorder()` ...

We've already seen one method of ordering a `data.table`, something like: `dt <- dt[order(varname),]`. But this requires copying the entire table, which is not memory efficient. To sort the table more efficiently, we can use this function:

```{r}
ltA
setorder(ltA,year,surname) #sort first by year, then by surname within each year
ltA
setorder(ltA,year,-surname) #Use a negative sign to reverse the ordering
ltA
```

### ... and more importantly, `setkey()`

There are computational advantages to having sorted tables. The reason is the same reason dictionaries put the words in alphabetical order and not randomly. When you search a dictionary for a word you (probably without thinking about it) implement a kind of [binary search](https://en.wikipedia.org/wiki/Binary_search_algorithm) algorithm. `data.table` is often asked to find values, in fact that's most of that the SELECT field does. If `data.table` knows that one or more columns are sorted, it can search them far more quickly. `setkey()` does this:

```{r}
setkey(ltA,forename,lap) #Note you cannot use `-` to reverse the order. That's why `setorder()` still exists
#This shows you that the data.table now has an attribute indicating which columns are sorted, or "keys", in the lingo
attributes(ltA)["sorted"]
setorder(ltA,-year)
#Obviously, if you reorder the rows later the keys will be removed.
attributes(ltA)["sorted"]
```

This becomes especially super important when performing very large merges, because these require a lot of searching behind the scenes to work out what rows match.

### `setnames()`

This is a good way to change column names. The syntax is `setnames(dataTable,c("vector","of","old","names"),c("VECTOR","OF","NEW","NAMES"))`

```{r}
colnames(newDt)
setnames(newDt,c("colCharacter","colInteger"),c("colChar","colInt"))
colnames(newDt)

# I like to use it in conjunction with magrittr pipes (if you don't get this command don't worry, magrittr can be a tutorial for another day)
require(magrittr)
newDt %>% setnames(colnames(.),LETTERS[1:(ncol(.))])
newDt
```

### `setDT()`

Similar to functions like `as.matrix()` and `as.data.frame()`, there is an `as.data.table()` function that will attempt to convert other objects into `data.table`s. This requires a lot of copying of data behind the scenes. Unless, of course, the object you are converting is a `data.frame` because as we discussed, a `data.table` is just an extension of a `data.frame`. To really bring that home:

```{r}
class(newDt) #See? It's classed as a data.table AND a data.frame
```

So, why bother copying? `setDT(dataFrame)` will instantly convert a `dataFrame` into a `data.table` with almost zero extra memory or processing required.

This and the other "set" functions above all save memory using this this zero-copying paradigm, so now is a good time to talk about that more generally.

#### Pass by reference, the key to speed and memory efficiency

**This is technical background**, feel free to skip it. But as you get more advanced with R, it will become very important.

Normally, when you pass a function an object in R, the function makes a copy of the object and works on the copy (oversimplified but, in general that's true). Copying data is inefficient, but can help programs to act "safely", since you are less likely to end up accidentally modifying an object you don't want to. This paradigm is known as *copy on modify semantics*.

`data.table` avoids making copies unless absolutely necessary. This is partly why it is so much faster and uses less memory than the same operations in tidyverse packages. It also bears some risks. Consider this function that involves adding a column to a `data.table`:

```{r}
addCol <- function(dt){
  dt[,newCol:=1:.N]
  return(NULL)
}
```

If we run this on `newDt` as defined above, nothing (well ... NULL) is returned. But look at the table before and after!

```{r}
colnames(newDt)
addCol(newDt)
colnames(newDt)
```

As you can see, even though we never specifically over-wrote our original table, it has been modified. that is because the walrus operator function (`:=`), like many `data.table` functions, uses *reference semantics*, that is, it is passed the memory address of the actual original table, which it them modifies. So you have to be careful. Other `data.table` operations will indeed copy part of or the whole table, but always aims to optimise speed and memory usage without being too risky. I mean ... there's no real harm in having an extra column floating around, but imagine if we deleted a column! That could actually be a problem. Be alert but not alarmed.

### Multiline DO_SOMETHING


DO_SOMETHING can get very complex. Use curly braces (`{` and `}`) to denote multi-command operations in there. I speculate that a driver's fastest lap of a race is usually a few laps before the last lap, when they are pushing the car hard but the tyres have not yet worn out too much. Let's check how many laps from the end of the race drivers normally complete their fastest lap of the race.

```{r}
fastLapDt <- lt[,{
  lastLap <- max(lap)
  verstappenFastest <- .SD[,lap[milliseconds==min(milliseconds)]]
  .( lapsBeforeLastFastest = verstappenFastest - lastLap )
},by=.(circuitId,year)]

hist( fastLapDt$lapsBeforeLastFastest, breaks=function(x){min(x):0} )
```

Yep, looks like it's most often the second to last lap, but by a tiny margin.

Take some time to make sure you understand every part of this command, it contains many tricks and is a good recap of the tutorial so far. 

Want a fun challenge? Figure out a way to do this with one line.

#### Use as a *for* loop

The GROUP_BY field effectively runs a set of commands for each group. When you think about it, that's quite similar to what a `for(){ ... }` loop does. R for-loops are [famously slow](https://towardsdatascience.com/r-is-slow-and-its-your-fault-2fcedacc7abb). There's a fascinating history as to why, it's partially on purpose to encourage you to use faster alternatives. [`*ply`-type functions](https://www.guru99.com/r-apply-sapply-tapply.html) are good, but using `data.table`s to mimic for-loops, as we have done above, is even faster.

You can even modify a variable outside the `data.table` while it is "looping", using the [`<<-` operator](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/assignOps).

## Getting extra tricky

### Special variable: `.GRP`

When you GROUP_BY, the groups are given a number you can access with the special variable .GRP. I can't think of a use case right now but remember it, it does occasionally come in very handy.

# Casting and melting

It's very common to want to **pivot** a table from wide to long form or back.

# Helpful functions

## foverlaps

## fifelse

## frank

## frollapply

## fintersect

## fcoalesce
